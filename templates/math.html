{% extends 'base.html' %}

{% block title_suffix %}{% endblock %}

{% block content %}
  <h1>{% block title %}Math{% endblock %}</h1>
  <p>
    The process of adjusting an image in the direction, that would improve the results, is called "gradient descent".
    Adjusting the image actually means adjusting the RGB values of each pixel.
    In mathematics, a gradient is the direction, in which a scalar function of multiple variables increases the fastest.
    It is a vector of the partial derivatives of the function with respect to each of its variables.
    In our case, the variables are the RGB values of each pixel.
    But when training the network itself, the variables are the network's parameters (filters).
    And the function is the "loss" or "cost" function, which measures the difference (or distance)
    between the desired output and the network's actual output at the current step.
    Again, it's an oversimplification - even for the same task,
    one could use different loss functions for different purposes,
    for example to tolerate or to penalise outliers in the output.
  </p>
  <p>
    The algorithm to compute the gradient, in tolerable time, is called "backpropagation".
    Because each layer's output is a function of the previous layer's output,
    this algorithm uses the "chain rule" of calculus and starts from the last layer,
    recursing backwards through the layers, and multiplying partial derivatives at each step.
    Hence the name "backpropagation". What's ingenious about it is that it only needs to traverse the network once.
    By itself, computing derivatives is a simple task in math.
    And the functions, computed by the layers are usually simple - (matrix) multiplications and additions.
    The convolutions (filters) in computer vision are "discrete" and boil down to multiplications and additions.
    But there are many of them. Also, modern neural networks have millions, or even many billions of parameters.
    This makes the task impossible for a human to do by hand, but in software, there exist tools to automate it.
    After the gradient is computed, all that is left to do is to adjust the parameters.
    And because the gradient is the direction of fastest increase of the cost function,
    we adjust the parameters in the opposite direction - by subtracting the gradient.
    Hence the name "gradient descent".
  </p>
  <p>
    It's like going downhill to return home when you're lost in a mountain'.
    It's dark and foggy, so you can't see, but you can feel the slope of the ground with your feet.
    Then you just follow the slope downward, step by step.
    Now, imagine that you are a super human and can take really large steps.
    You're also hungry, so you want to return home fast!
    You may save time by taking huge steps, but if you may also end up on the hill on the other side of your home.
    Same for gradient descent - we only know the first partial derivatives,
    so we need to take moderate steps to avoid overshooting the minimum of the cost function.
  </p>
  <p>
    This example is only two-dimensional (the height of the ground being the 3rd dimension).
    Neural networks have many parameters, so their "home" is in much higher-dimensional space.
    We can't imagine or visualize anything above 3D (or 4D if you imagine time progressing),
    but for the network it's just some more numbers.
    And it turns out that working in a high-dimensional space even has benefits,
    because many of our intuitions from low-dimensional spaces are no longer valid.
    For example, there is a very low probability that we would end up in a local minimum,
    because this probability decreases exponentially with the number of dimensions.
  </p>
  <p><a href="{{ url_for('index') }}">Home</a></p>
{% endblock %}
